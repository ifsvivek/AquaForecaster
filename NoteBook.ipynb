{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-1E4KjUz-u_A",
    "outputId": "4e0628d4-a721-465d-f700-885a127843de"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"DATA.xlsx\"  # Replace with your dataset file path\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Display the dataset structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_ZAQuLN_O_0"
   },
   "outputs": [],
   "source": [
    "# Step 1: Prepare input features (X) and output targets (y)\n",
    "# Check the number of columns in your DataFrame\n",
    "num_cols = data.shape[1]\n",
    "\n",
    "# Adjust slicing to ensure you select at least one column for X\n",
    "X = data.iloc[:, : num_cols - 10]  # Inputs: All columns except the last 10\n",
    "# If data has less than 10 columns the previous line will make X to have 0 columns.\n",
    "# the next line, in that case, will assign all columns but the last one to X.\n",
    "if X.shape[1] == 0:\n",
    "    X = data.iloc[:, :-1]\n",
    "\n",
    "y = data.iloc[\n",
    "    :, -10:\n",
    "]  # Outputs: Last 10 columns (NO3, Th, Ca, Mg, Cl, TDS, F, pH, Cr, Fe)\n",
    "# If data has less than 10 columns the previous line will make y to have 0 columns.\n",
    "# the next line, in that case, will assign only the last column to y.\n",
    "if y.shape[1] == 0:\n",
    "    y = data.iloc[:, -1:]\n",
    "\n",
    "# Step 2: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Scale the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0YL5vgL_kic",
    "outputId": "54e59336-01a0-4857-c7d9-18843581fde0"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "# Build the enhanced ANN Model\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Dense(units=256, activation=\"relu\", input_dim=X_train_scaled.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Hidden Layers\n",
    "model.add(Dense(units=128, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=32, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=16, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(units=y_train_scaled.shape[1], activation=\"linear\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])\n",
    "\n",
    "# Train with reasonable batch size\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_scaled,\n",
    "    epochs=10000,\n",
    "    batch_size=99999999999,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Evaluate and save\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test_scaled)\n",
    "model.save(\"water_quality_ann_enhanced.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "R6QLCtYC_sR-",
    "outputId": "7a2b1f76-06bc-47b8-c7e1-b29e6af6d42a"
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.savefig(\"training_validation_loss.png\")  # Save the plot as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "model = load_model(\"water_quality_ann_enhanced.h5\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Adjust input_count based on model's expected input shape\n",
    "input_count = 9  # Set to 9 as per model's input requirement\n",
    "target_count = 10  # Number of output features\n",
    "\n",
    "# Ensure the dataset has enough columns\n",
    "if data.shape[1] < input_count:\n",
    "    raise ValueError(\n",
    "        f\"Dataset must have at least {input_count} columns, but it has {data.shape[1]}\"\n",
    "    )\n",
    "\n",
    "# Extract input features used during training\n",
    "historical_data = data.iloc[:, :input_count]\n",
    "\n",
    "# Prepare the scalers (assuming they are already fitted)\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Fit the scalers on historical data (if not already fitted)\n",
    "scaler_X.fit(historical_data)\n",
    "scaler_y.fit(data.iloc[:, :target_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predict_future function\n",
    "def predict_future(data, scaler_X, scaler_y, input_count, target_count, months):\n",
    "    predictions = []\n",
    "    future_data = data.copy()\n",
    "\n",
    "    for _ in range(months):\n",
    "        # Scale the input data\n",
    "        future_scaled = scaler_X.transform(future_data)  # Shape: [1, input_count]\n",
    "\n",
    "        # Make prediction\n",
    "        prediction_scaled = model.predict(future_scaled, verbose=0)\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        prediction_original = scaler_y.inverse_transform(prediction_scaled)\n",
    "\n",
    "        # Append prediction to the list\n",
    "        predictions.append(prediction_original[0])\n",
    "\n",
    "        # Prepare the next input using the first 'input_count' predicted features\n",
    "        future_data = pd.DataFrame(\n",
    "            [prediction_original[0][:input_count]],\n",
    "            columns=future_data.columns[:input_count],\n",
    "        )\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last known data\n",
    "last_known_data = historical_data.tail(1)\n",
    "\n",
    "# Predict the next 2 months\n",
    "predictions = predict_future(\n",
    "    last_known_data, scaler_X, scaler_y, input_count, target_count, months=2\n",
    ")\n",
    "output_columns = data.columns[:target_count]\n",
    "# Display Predictions\n",
    "for i, prediction in enumerate(predictions, start=1):\n",
    "    print(f\"Month {i}:\")\n",
    "    for col_name, value in zip(output_columns, prediction):\n",
    "        print(f\" {col_name}: {value:.2f}\")\n",
    "    print(\"\")  # Add an empty line between months"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
